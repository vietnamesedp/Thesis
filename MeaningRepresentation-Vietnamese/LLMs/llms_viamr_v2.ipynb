{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT THƯ VIỆN VÀ LLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import time, json\n",
    "import google.generativeai as genai\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001-tuning\n"
     ]
    }
   ],
   "source": [
    "api_key_trung = \"secret_key\"\n",
    "client = OpenAI(\n",
    "    api_key=api_key_trung,\n",
    ")\n",
    "\n",
    "GOOGLE_API_KEY = 'secret_key'\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 252, 369)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/Amr_v4/train_v5.txt', 'rt') as f:\n",
    "    train = f.read().split('\\n\\n')\n",
    "    train_sentences = [_ for _ in train if _.startswith('#::')]\n",
    "    train_viamr = [_ for _ in train if not _.startswith(\"#::\")]\n",
    "\n",
    "with open('./data/Amr_v4/test_v5.txt', 'rt') as f:\n",
    "    test = f.read().split('\\n')\n",
    "\n",
    "len(train_sentences), len(train_viamr), len(test)\n",
    "\n",
    "# for i, j in zip(train_sentences, train_viamr):\n",
    "#     print(i, j, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v1 = lambda input_sentence, train_ex: f\"\"\"\n",
    "Hãy chuyển đổi câu sau sang định dạng AMR:\n",
    "\n",
    "```\n",
    "{input_sentence}\n",
    "```\n",
    "\n",
    "Dưới đây là các ví dụ cụ thể trong đó:\n",
    "* dòng có ký hiệu ::snt hoặc #snt đầu tiên là chỉ câu input đầu vào của người dùng (tập trung vào phần sau của :: snt)\n",
    "* đoạn còn lại thì là kết quả sau khi chuyển một câu thành định dạng AMR (để ý kỹ định dạng và cách đưa input sang dạng AMR)\n",
    "\n",
    "Ví dụ:\n",
    "```\n",
    "{train_ex}\n",
    "```\n",
    "Lưu ý:\n",
    "\n",
    "* Hãy đảm bảo rằng định dạng AMR được giữ nguyên.\n",
    "* Hãy sử dụng các ký hiệu chính xác trong AMR.\n",
    "* Hãy đảm bảo rằng các từ trong AMR được ánh xạ chính xác với các từ trong câu đầu vào.\n",
    "\"\"\"\n",
    "\n",
    "train_all = '\\n\\n'.join(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_4o(sent: str):\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_v1(sent, train_all)\n",
    "            },\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    result = chat_completion.choices[0].message.content\n",
    "    status = True if result else False\n",
    "    if not status:\n",
    "        raise RuntimeError\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output = []\n",
    "for t in test:\n",
    "    _ = gpt_4o(t)\n",
    "    gpt_output.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./llms_output/amr_v4/v5_gpt_4o_few.txt', 'wt') as f:\n",
    "    for out in gpt_output:\n",
    "        f.write(out + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "GEMINI_FLAG = '<Error>'\n",
    "\n",
    "def gemini_viamr(sent: str):\n",
    "    \n",
    "    promt = prompt_v1(sent, train_ex = train_all)\n",
    "    res = model.generate_content(promt)\n",
    "\n",
    "    return res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_output = []\n",
    "for t in test:\n",
    "    try:\n",
    "        _ = gemini_viamr(t)\n",
    "    except:\n",
    "        _ = GEMINI_FLAG\n",
    "\n",
    "    gemini_output.append(_) \n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 26)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gemini_output), gemini_output.count(GEMINI_FLAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc68c3c434a34747b78463a0547b84c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (t, out) in tqdm(enumerate(zip(test, gemini_output)), total= gemini_output.count(GEMINI_FLAG)):\n",
    "    if out == GEMINI_FLAG:\n",
    "        try:\n",
    "            _ = gemini_viamr(t)\n",
    "        except:\n",
    "            _ = GEMINI_FLAG\n",
    "        \n",
    "        gemini_output[i] = _\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./llms_output/amr_v4/v5_gemini_outputv1.txt', 'wt') as f:\n",
    "    for i in gemini_output:\n",
    "        f.write(i + '\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
