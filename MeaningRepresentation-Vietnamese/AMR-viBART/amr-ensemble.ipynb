{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7d253e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:16:28.730384Z",
     "iopub.status.busy": "2024-08-28T06:16:28.729459Z",
     "iopub.status.idle": "2024-08-28T06:16:53.689127Z",
     "shell.execute_reply": "2024-08-28T06:16:53.687825Z"
    },
    "id": "t_zJ5g-UGtyG",
    "outputId": "e30c76a7-1aeb-4e4c-92b6-2a58f8fa29ee",
    "papermill": {
     "duration": 24.974368,
     "end_time": "2024-08-28T06:16:53.692075",
     "exception": false,
     "start_time": "2024-08-28T06:16:28.717707",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.0)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4224f38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:16:53.717658Z",
     "iopub.status.busy": "2024-08-28T06:16:53.717200Z",
     "iopub.status.idle": "2024-08-28T06:17:26.712968Z",
     "shell.execute_reply": "2024-08-28T06:17:26.711707Z"
    },
    "id": "f23fVStvFF2N",
    "outputId": "8adeb18b-ebf5-4982-9497-cc4fd1ca93a0",
    "papermill": {
     "duration": 33.011692,
     "end_time": "2024-08-28T06:17:26.715773",
     "exception": false,
     "start_time": "2024-08-28T06:16:53.704081",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Penman\r\n",
      "  Downloading penman-1.3.1-py3-none-any.whl.metadata (7.7 kB)\r\n",
      "Collecting unidecode\r\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting amrlib==0.5.1\r\n",
      "  Downloading amrlib-0.5.1-py3-none-any.whl.metadata (5.5 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from amrlib==0.5.1) (1.26.4)\r\n",
      "Collecting smatch (from amrlib==0.5.1)\r\n",
      "  Downloading smatch-1.0.4.tar.gz (26 kB)\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.0 in /opt/conda/lib/python3.10/site-packages (from amrlib==0.5.1) (3.7.5)\r\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from amrlib==0.5.1) (2.1.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from amrlib==0.5.1) (4.66.4)\r\n",
      "Requirement already satisfied: transformers>=3.0 in /opt/conda/lib/python3.10/site-packages (from amrlib==0.5.1) (4.42.3)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (8.2.3)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (0.12.3)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (2.5.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (69.0.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=2.0->amrlib==0.5.1) (3.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->amrlib==0.5.1) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->amrlib==0.5.1) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->amrlib==0.5.1) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->amrlib==0.5.1) (3.2.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->amrlib==0.5.1) (2024.5.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0->amrlib==0.5.1) (0.23.4)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0->amrlib==0.5.1) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0->amrlib==0.5.1) (2023.12.25)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0->amrlib==0.5.1) (0.4.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0->amrlib==0.5.1) (0.19.1)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.0->amrlib==0.5.1) (1.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy>=2.0->amrlib==0.5.1) (3.1.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0->amrlib==0.5.1) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0->amrlib==0.5.1) (2.14.6)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0->amrlib==0.5.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0->amrlib==0.5.1) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0->amrlib==0.5.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0->amrlib==0.5.1) (2024.7.4)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0->amrlib==0.5.1) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0->amrlib==0.5.1) (0.1.4)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0->amrlib==0.5.1) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0->amrlib==0.5.1) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy>=2.0->amrlib==0.5.1) (13.7.0)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0->amrlib==0.5.1) (0.18.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.0->amrlib==0.5.1) (6.4.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy>=2.0->amrlib==0.5.1) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->amrlib==0.5.1) (1.3.0)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.0->amrlib==0.5.1) (1.1.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0->amrlib==0.5.1) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0->amrlib==0.5.1) (2.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.0->amrlib==0.5.1) (0.1.2)\r\n",
      "Downloading amrlib-0.5.1-py3-none-any.whl (297 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading penman-1.3.1-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: smatch\r\n",
      "  Building wheel for smatch (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for smatch: filename=smatch-1.0.4-py3-none-any.whl size=24054 sha256=953bf20a543d266ca054eeb660a14ccba37740c9f64490b1b8e14dcf4cf925f1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/5e/2d/61b88bc74b337fbf3e998b9aa6b43bec72227e18a84a8335e8\r\n",
      "Successfully built smatch\r\n",
      "Installing collected packages: smatch, unidecode, Penman, amrlib\r\n",
      "Successfully installed Penman-1.3.1 amrlib-0.5.1 smatch-1.0.4 unidecode-1.3.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install Penman unidecode amrlib==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36787ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:26.747903Z",
     "iopub.status.busy": "2024-08-28T06:17:26.747475Z",
     "iopub.status.idle": "2024-08-28T06:17:26.752509Z",
     "shell.execute_reply": "2024-08-28T06:17:26.751564Z"
    },
    "id": "wAAhmIDGHOOg",
    "papermill": {
     "duration": 0.023471,
     "end_time": "2024-08-28T06:17:26.754664",
     "exception": false,
     "start_time": "2024-08-28T06:17:26.731193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/IBM/graph_ensemble_learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ee38cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:26.786599Z",
     "iopub.status.busy": "2024-08-28T06:17:26.786248Z",
     "iopub.status.idle": "2024-08-28T06:17:26.790739Z",
     "shell.execute_reply": "2024-08-28T06:17:26.789762Z"
    },
    "id": "wHnSB4-gIC9s",
    "papermill": {
     "duration": 0.02381,
     "end_time": "2024-08-28T06:17:26.793044",
     "exception": false,
     "start_time": "2024-08-28T06:17:26.769234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cp -r graph_ensemble_learning/.* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a37312c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:26.825125Z",
     "iopub.status.busy": "2024-08-28T06:17:26.824742Z",
     "iopub.status.idle": "2024-08-28T06:17:27.948231Z",
     "shell.execute_reply": "2024-08-28T06:17:27.946711Z"
    },
    "papermill": {
     "duration": 1.1427,
     "end_time": "2024-08-28T06:17:27.950951",
     "exception": false,
     "start_time": "2024-08-28T06:17:26.808251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -r *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e23eb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:27.982501Z",
     "iopub.status.busy": "2024-08-28T06:17:27.982093Z",
     "iopub.status.idle": "2024-08-28T06:17:29.188532Z",
     "shell.execute_reply": "2024-08-28T06:17:29.187119Z"
    },
    "papermill": {
     "duration": 1.225883,
     "end_time": "2024-08-28T06:17:29.191491",
     "exception": false,
     "start_time": "2024-08-28T06:17:27.965608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/amr-data/graph_ensemble_learning_new/* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d5d5a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:29.224265Z",
     "iopub.status.busy": "2024-08-28T06:17:29.223829Z",
     "iopub.status.idle": "2024-08-28T06:17:30.401101Z",
     "shell.execute_reply": "2024-08-28T06:17:30.399820Z"
    },
    "papermill": {
     "duration": 1.197033,
     "end_time": "2024-08-28T06:17:30.403958",
     "exception": false,
     "start_time": "2024-08-28T06:17:29.206925",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\r\n",
      "import os\r\n",
      "import random\r\n",
      "import warnings\r\n",
      "from pathlib import Path\r\n",
      "\r\n",
      "import penman\r\n",
      "import torch\r\n",
      "from amrlib.evaluate.smatch_enhanced import get_entries, compute_smatch\r\n",
      "from amrlib.utils.logging import silence_penman\r\n",
      "from penman.models.noop import NoOpModel\r\n",
      "from torch import optim\r\n",
      "from torch.utils.data import DataLoader\r\n",
      "from tqdm import tqdm\r\n",
      "\r\n",
      "from amr_utils.datasets.dataset import AMRPenman, AMRPenmanMultiTasks, add_prefix, CombineData, AMR_GENERATION\r\n",
      "from .inference import Inference\r\n",
      "from ..models.lg import LG\r\n",
      "\r\n",
      "import gc\r\n",
      "\r\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
      "warnings.simplefilter('ignore')\r\n",
      "\r\n",
      "\r\n",
      "def save(optimizer, model, path):\r\n",
      "    Path(path).mkdir(exist_ok=True)\r\n",
      "    with open(os.path.join(path, f'multitask.model'), 'wb+') as f:\r\n",
      "        if torch.cuda.device_count() > 1:\r\n",
      "            torch.save({\r\n",
      "                'model_state_dict': model.model.module.state_dict(),\r\n",
      "                'optimizer_state_dict': optimizer.state_dict(),\r\n",
      "            }, f)\r\n",
      "        else:\r\n",
      "            torch.save({\r\n",
      "                'model_state_dict': model.model.state_dict(),\r\n",
      "                'optimizer_state_dict': optimizer.state_dict(),\r\n",
      "            }, f)\r\n",
      "\r\n",
      "\r\n",
      "def train(epoch, batch_size, locations, data_types, task_types):\r\n",
      "    datasets = []\r\n",
      "    i = 0\r\n",
      "    for location, data_type in zip(locations, data_types):\r\n",
      "        datasets.append(AMRPenmanMultiTasks(location, task_types[i]))\r\n",
      "\r\n",
      "    data_set = CombineData(datasets)\r\n",
      "\r\n",
      "    train_set = DataLoader(\r\n",
      "        data_set, batch_size=batch_size, num_workers=4, shuffle=True\r\n",
      "    )\r\n",
      "\r\n",
      "    dataset = iter(train_set)\r\n",
      "    pbar = tqdm(dataset)\r\n",
      "    moving_loss = 0\r\n",
      "    net.train()\r\n",
      "    n = 1\r\n",
      "    for source, target in pbar:\r\n",
      "        net.zero_grad()\r\n",
      "\r\n",
      "        # text to amr\r\n",
      "        loss = net(source, target)\r\n",
      "        loss.backward()\r\n",
      "        optimizer.step()\r\n",
      "        moving_loss += loss.cpu().item()\r\n",
      "\r\n",
      "        pbar.set_description(\r\n",
      "            'Epoch: {}; Loss: {:.5f}; Moving Loss: {:.5f};'.format(\r\n",
      "                epoch + 1, loss.cpu().item(), moving_loss / n\r\n",
      "            )\r\n",
      "        )\r\n",
      "        n += 1\r\n",
      "\r\n",
      "        if args.pickle_steps > 0 and n % args.pickle_steps == 0:\r\n",
      "            print(\"Saving model to\", args.output)\r\n",
      "            save(optimizer, net, args.output)\r\n",
      "    \r\n",
      "    torch.cuda.empty_cache()\r\n",
      "    gc.collect()\r\n",
      "\r\n",
      "\r\n",
      "def valid(epoch, batch_size, root, num_beams, num_ret_seq, output_dir=None, prefix='dev'):\r\n",
      "    silence_penman()\r\n",
      "    data_set = AMRPenman(root)\r\n",
      "\r\n",
      "    valid_set = DataLoader(\r\n",
      "        data_set, batch_size=2 * batch_size, num_workers=4, shuffle=False\r\n",
      "    )\r\n",
      "    dataset = iter(valid_set)\r\n",
      "    avg_loss = 0.0\r\n",
      "    n = 0.0\r\n",
      "    net.eval()\r\n",
      "    inference = Inference(net)\r\n",
      "    amrs = []\r\n",
      "    amr_predictions = []\r\n",
      "    with torch.no_grad():\r\n",
      "        for amr_seq, sentence, amr in tqdm(dataset):\r\n",
      "            gids = []\r\n",
      "            for a in amr:\r\n",
      "                g = penman.decode(a, model=NoOpModel())\r\n",
      "                if 'id' in g.metadata:\r\n",
      "                    gid = g.metadata['id']\r\n",
      "                else:\r\n",
      "                    gid = None\r\n",
      "                gids.append(gid)\r\n",
      "            loss = net(add_prefix(sentence, AMR_GENERATION), amr_seq)\r\n",
      "            avg_loss += loss.cpu().item()\r\n",
      "            n += 1\r\n",
      "            predict_amrs = inference.parse_sentences(add_prefix(sentence, AMR_GENERATION), num_beams, num_ret_seq)\r\n",
      "            amrs += amr\r\n",
      "            ps = []\r\n",
      "            for i in range(len(predict_amrs)):\r\n",
      "                p = predict_amrs[i]\r\n",
      "                gid = gids[i]\r\n",
      "                s = sentence[i]\r\n",
      "                if gid is None:\r\n",
      "                    gid = \" =======heheheheh =====\"\r\n",
      "                if p is not None:\r\n",
      "                    ps.append('# ::snt ' + s + '\\n' + '# ::id ' + gid + '\\n' + p)\r\n",
      "                else:\r\n",
      "                    ps.append(None)\r\n",
      "            predict_amrs = ps\r\n",
      "            amr_predictions += predict_amrs\r\n",
      "\r\n",
      "    torch.cuda.empty_cache()\r\n",
      "    gc.collect()\r\n",
      "\r\n",
      "    num_none = len([p for p in amr_predictions if p is None])\r\n",
      "    print(\"Number of None:\", num_none)\r\n",
      "\r\n",
      "    if output_dir is not None:\r\n",
      "        ref_out_fn = prefix + '.txt.reference'\r\n",
      "        gen_out_fn = prefix + '.txt.generated'\r\n",
      "        ref_fname = os.path.join(output_dir, ref_out_fn)\r\n",
      "        gen_fname = os.path.join(output_dir, gen_out_fn)\r\n",
      "        f_ref = open(ref_fname, 'w')\r\n",
      "        f_gen = open(gen_fname, 'w')\r\n",
      "        print('Saving %s and %s' % (ref_fname, gen_fname))\r\n",
      "        skipped = 0\r\n",
      "        for ref_graph, gen_graph in zip(amrs, amr_predictions):\r\n",
      "            if gen_graph is None:\r\n",
      "                skipped += 1\r\n",
      "                continue\r\n",
      "            f_ref.write(ref_graph + '\\n\\n')\r\n",
      "            f_gen.write(gen_graph + '\\n\\n')\r\n",
      "        f_ref.close()\r\n",
      "        f_gen.close()\r\n",
      "        print('Out of %d graphs, skipped %d that did not deserialize properly.' % (len(amrs), skipped))\r\n",
      "        print()\r\n",
      "        gold_entries = get_entries(ref_fname)\r\n",
      "        test_entries = get_entries(gen_fname)\r\n",
      "        precision, recall, f_score = compute_smatch(test_entries, gold_entries)\r\n",
      "        print(epoch + 1, ' SMATCH -> P: %.3f,  R: %.3f,  F: %.3f' % (precision, recall, f_score))\r\n",
      "    else:\r\n",
      "        precision, recall, f_score = None, None, None\r\n",
      "\r\n",
      "    print(\r\n",
      "        'Validation Epoch: {}; Avg Loss: {:.5f};'.format(\r\n",
      "            epoch + 1, avg_loss / n\r\n",
      "        )\r\n",
      "    )\r\n",
      "\r\n",
      "    torch.cuda.empty_cache()\r\n",
      "    gc.collect()\r\n",
      "\r\n",
      "    return avg_loss / n, -f_score\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    parser = argparse.ArgumentParser(description='AMR training')\r\n",
      "    parser.add_argument('-t', '--train', type=str, help='Root directory with the training datasets', required=True)\r\n",
      "    parser.add_argument('-v', '--validation', type=str, help='Root directory with the validation datasets',  required=True)\r\n",
      "    parser.add_argument('-r', '--report_test', type=str, help='Root directory with the test datasets', required=True)\r\n",
      "    parser.add_argument('-l', '--lr', type=float, default=1e-4, help='Learning rate')\r\n",
      "    parser.add_argument('-e', '--epochs', type=int, default=30, help='The number of epochs')\r\n",
      "    parser.add_argument('-b', '--batch', type=int, default=4, help='Mini-batch size')\r\n",
      "    parser.add_argument('-o', '--output', type=str, default='./',\r\n",
      "                        help='Output folder where the model, and output files will be pickled')\r\n",
      "    parser.add_argument('-m', '--model', type=str, default='t5-base', help='model name')\r\n",
      "    parser.add_argument('--max_source_length', type=int, default=16, help='Max source length')\r\n",
      "    parser.add_argument('--max_target_length', type=int, default=16, help='Max target length')\r\n",
      "    parser.add_argument('-c', '--checkpoint', type=str, default=None, help='Checkpoint model')\r\n",
      "    parser.add_argument('-p', '--pickle_steps', type=int, default=-1,\r\n",
      "                        help='Save the model after every pickle_steps of mini-batches')\r\n",
      "    parser.add_argument('--num_beams', type=int, default=5, help='Number of beams used during inference')\r\n",
      "    parser.add_argument('--num_ret_seq', type=int, default=5,\r\n",
      "                        help='Number of return sequences for each prediction during inference')\r\n",
      "    parser.add_argument('--model_type', type=str, default='t5', help='Model type: bart or t5')\r\n",
      "    parser.add_argument('--data_type', default=\"blinkify\", help='Type of datasets')\r\n",
      "    parser.add_argument('--task_type', default=\"text2amr\", help='Task name')\r\n",
      "    parser.add_argument('--val_from_epoch', type=int, default=3, help='Only validation from epochs')\r\n",
      "    parser.add_argument('--do_val', default='yes', help='Whether do validation')\r\n",
      "    parser.add_argument('--early_termination', default=7, type=int, help='Number of epochs before early termination')\r\n",
      "    parser.add_argument('--random_seed', default=0, type=int, help='Default random seed')\r\n",
      "\r\n",
      "    args = parser.parse_args()\r\n",
      "\r\n",
      "    torch.manual_seed(args.random_seed)\r\n",
      "    random.seed(args.random_seed)\r\n",
      "\r\n",
      "    net = LG(args.model,\r\n",
      "             max_source_length=args.max_source_length,\r\n",
      "             max_target_length=args.max_target_length,\r\n",
      "             model_type=args.model_type)\r\n",
      "\r\n",
      "    optimizer = optim.Adam(net.parameters(), lr=args.lr)\r\n",
      "    if args.checkpoint is not None:\r\n",
      "        print(\"Load model from \", args.checkpoint)\r\n",
      "        checkpoint = torch.load(args.checkpoint)\r\n",
      "        if torch.cuda.device_count() <= 1:\r\n",
      "            net.model.load_state_dict(checkpoint['model_state_dict'])\r\n",
      "        else:\r\n",
      "            net.model.module.load_state_dict(checkpoint['model_state_dict'])\r\n",
      "\r\n",
      "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
      "        net.train()\r\n",
      "\r\n",
      "    best_so_far = None\r\n",
      "    early_termination = 0\r\n",
      "    val_from_epoch = args.val_from_epoch\r\n",
      "    for epoch in range(args.epochs):\r\n",
      "        train(epoch, args.batch, args.train.split(), args.data_type.split(), args.task_type.split())\r\n",
      "        if args.do_val == 'yes':\r\n",
      "            if epoch > val_from_epoch:\r\n",
      "                print(\"Validation\")\r\n",
      "                _, score = valid(epoch,\r\n",
      "                                 args.batch,\r\n",
      "                                 args.validation,\r\n",
      "                                 args.num_beams,\r\n",
      "                                 args.num_ret_seq,\r\n",
      "                                 output_dir=args.output,\r\n",
      "                                 prefix='dev_')\r\n",
      "                if best_so_far is None:\r\n",
      "                    best_so_far = score\r\n",
      "                    print(\"Saving model to\", args.output)\r\n",
      "                    save(optimizer, net, args.output)\r\n",
      "                elif best_so_far > score:\r\n",
      "                    print('Validation score has improved from', best_so_far, ' to ', score)\r\n",
      "                    best_so_far = score\r\n",
      "                    early_termination = 0\r\n",
      "                    print(\"Saving model to\", args.output)\r\n",
      "                    save(optimizer, net, args.output)\r\n",
      "                    valid(epoch,\r\n",
      "                          args.batch,\r\n",
      "                          args.report_test,\r\n",
      "                          args.num_beams,\r\n",
      "                          args.num_ret_seq,\r\n",
      "                          args.output,\r\n",
      "                          prefix='test_')\r\n",
      "                else:\r\n",
      "                    early_termination += 1\r\n",
      "                if early_termination > args.early_termination:\r\n",
      "                    break\r\n",
      "\r\n",
      "    # if args.do_val != 'yes':\r\n",
      "    save(optimizer, net, args.output)\r\n"
     ]
    }
   ],
   "source": [
    "!cat amr_parsing/t5/cli/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e1bff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:30.437931Z",
     "iopub.status.busy": "2024-08-28T06:17:30.437479Z",
     "iopub.status.idle": "2024-08-28T06:17:31.579340Z",
     "shell.execute_reply": "2024-08-28T06:17:31.578232Z"
    },
    "papermill": {
     "duration": 1.162212,
     "end_time": "2024-08-28T06:17:31.582148",
     "exception": false,
     "start_time": "2024-08-28T06:17:30.419936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  amr_parsing  amr_utils  ensemble\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16722b89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:31.615690Z",
     "iopub.status.busy": "2024-08-28T06:17:31.615271Z",
     "iopub.status.idle": "2024-08-28T06:17:31.621197Z",
     "shell.execute_reply": "2024-08-28T06:17:31.619947Z"
    },
    "id": "_qp-du_RIJpJ",
    "outputId": "0f896b4c-98fc-441c-9f62-c4ea37102ba7",
    "papermill": {
     "duration": 0.025546,
     "end_time": "2024-08-28T06:17:31.623522",
     "exception": false,
     "start_time": "2024-08-28T06:17:31.597976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -r DATA_AMR_temp/\n",
    "# !rm -r LDC2017T10/\n",
    "\n",
    "# !mkdir -p DATA_AMR_temp/training/\n",
    "# !mkdir -p DATA_AMR_temp/dev/\n",
    "# !mkdir -p DATA_AMR_temp/test/\n",
    "\n",
    "# # !cp -r /content/drive/MyDrive/AMR_data/amr-bank-struct-v1.6.txt ./DATA_AMR_temp/training/\n",
    "# # !cp -r /content/drive/MyDrive/AMR_data/amr-bank-struct-v1.6.txt ./DATA_AMR_temp/dev/\n",
    "# # !cp -r /content/drive/MyDrive/AMR_data/amr-bank-struct-v1.6.txt ./DATA_AMR_temp/test/\n",
    "\n",
    "# # !cp -r /kaggle/input/amr-data/vn_data/vn_data/train_amr_v1_en_form.txt ./DATA_AMR_temp/training\n",
    "# # !cp -r /kaggle/input/amr-data/vn_data/vn_data/train_amr_v1_en_form.txt ./DATA_AMR_temp/dev\n",
    "# # !cp -r /kaggle/input/amr-data/vn_data/vn_data/train_amr_v1_en_form.txt ./DATA_AMR_temp/test\n",
    "\n",
    "# !cp -r /kaggle/input/amr-data/train_amr_v3_en_form.txt ./DATA_AMR_temp/training\n",
    "# !cp -r /kaggle/input/amr-data/train_amr_v3_en_form.txt ./DATA_AMR_temp/dev\n",
    "# !cp -r /kaggle/input/amr-data/train_amr_v3_en_form.txt ./DATA_AMR_temp/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac81aeb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:31.656537Z",
     "iopub.status.busy": "2024-08-28T06:17:31.656146Z",
     "iopub.status.idle": "2024-08-28T06:17:31.661007Z",
     "shell.execute_reply": "2024-08-28T06:17:31.659985Z"
    },
    "id": "TwMlbQ-lO09e",
    "papermill": {
     "duration": 0.024111,
     "end_time": "2024-08-28T06:17:31.663391",
     "exception": false,
     "start_time": "2024-08-28T06:17:31.639280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88951630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:31.697354Z",
     "iopub.status.busy": "2024-08-28T06:17:31.696562Z",
     "iopub.status.idle": "2024-08-28T06:17:31.701785Z",
     "shell.execute_reply": "2024-08-28T06:17:31.700712Z"
    },
    "id": "9MBzgXqzOe2k",
    "papermill": {
     "duration": 0.024677,
     "end_time": "2024-08-28T06:17:31.704095",
     "exception": false,
     "start_time": "2024-08-28T06:17:31.679418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def convert_data(file_name):\n",
    "#   with open(file_name, \"r\") as f:\n",
    "#     content = f.read()\n",
    "#     f.close()\n",
    "\n",
    "#   content = re.sub(r\"#::\", \"# ::\", content)\n",
    "\n",
    "#   content = re.sub(r\"\\n\\n\", \"\\n\", content)\n",
    "    \n",
    "#   if \"v3\" in file_name: content = re.sub(r\"# ::snt\", \"\\n# ::snt\", content)\n",
    "\n",
    "#   with open(file_name, \"w\") as f:\n",
    "#     f.write(content)\n",
    "#     f.close()\n",
    "\n",
    "# convert_data(\"./DATA_AMR_temp/training/train_amr_v3_en_form.txt\")\n",
    "# convert_data(\"./DATA_AMR_temp/dev/train_amr_v3_en_form.txt\")\n",
    "# convert_data(\"./DATA_AMR_temp/test/train_amr_v3_en_form.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c070a464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:31.736797Z",
     "iopub.status.busy": "2024-08-28T06:17:31.736401Z",
     "iopub.status.idle": "2024-08-28T06:17:31.740884Z",
     "shell.execute_reply": "2024-08-28T06:17:31.739933Z"
    },
    "id": "riwHDGlxIrNK",
    "outputId": "0aaa18c9-13ce-4d37-c9ff-835be7a31889",
    "papermill": {
     "duration": 0.023769,
     "end_time": "2024-08-28T06:17:31.743277",
     "exception": false,
     "start_time": "2024-08-28T06:17:31.719508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! python -u -m amr_utils.preprocess.preprocess_amr -i ./DATA_AMR_temp/ -o LDC2017T10/preprocessed_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab61ff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:31.775312Z",
     "iopub.status.busy": "2024-08-28T06:17:31.774935Z",
     "iopub.status.idle": "2024-08-28T06:17:32.934011Z",
     "shell.execute_reply": "2024-08-28T06:17:32.932658Z"
    },
    "id": "lXBQyRLGgetE",
    "papermill": {
     "duration": 1.178454,
     "end_time": "2024-08-28T06:17:32.936919",
     "exception": false,
     "start_time": "2024-08-28T06:17:31.758465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p t5_amr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1401ff16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:32.969477Z",
     "iopub.status.busy": "2024-08-28T06:17:32.969077Z",
     "iopub.status.idle": "2024-08-28T06:17:32.974678Z",
     "shell.execute_reply": "2024-08-28T06:17:32.973781Z"
    },
    "papermill": {
     "duration": 0.024667,
     "end_time": "2024-08-28T06:17:32.976984",
     "exception": false,
     "start_time": "2024-08-28T06:17:32.952317",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cat ./LDC2017T10/preprocessed_data/train.txt.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "169d4c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:33.009600Z",
     "iopub.status.busy": "2024-08-28T06:17:33.008676Z",
     "iopub.status.idle": "2024-08-28T06:17:35.277538Z",
     "shell.execute_reply": "2024-08-28T06:17:35.276117Z"
    },
    "papermill": {
     "duration": 2.288442,
     "end_time": "2024-08-28T06:17:35.280424",
     "exception": false,
     "start_time": "2024-08-28T06:17:32.991982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/amr-data/train_v4.jsonl ./\n",
    "!cp -r /kaggle/input/amr-data/test_v4.txt ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34d0135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:35.315134Z",
     "iopub.status.busy": "2024-08-28T06:17:35.314714Z",
     "iopub.status.idle": "2024-08-28T06:17:36.459786Z",
     "shell.execute_reply": "2024-08-28T06:17:36.458475Z"
    },
    "papermill": {
     "duration": 1.165691,
     "end_time": "2024-08-28T06:17:36.462797",
     "exception": false,
     "start_time": "2024-08-28T06:17:35.297106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  amr_utils  t5_amr\t    train_v4.jsonl\r\n",
      "amr_parsing\t    ensemble   test_v4.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d0ce73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:36.496998Z",
     "iopub.status.busy": "2024-08-28T06:17:36.496266Z",
     "iopub.status.idle": "2024-08-28T06:17:37.629395Z",
     "shell.execute_reply": "2024-08-28T06:17:37.628101Z"
    },
    "id": "oaghc-v1I25r",
    "outputId": "9db320f7-7c3d-44af-e730-ca582eb64357",
    "papermill": {
     "duration": 1.153376,
     "end_time": "2024-08-28T06:17:37.632162",
     "exception": false,
     "start_time": "2024-08-28T06:17:36.478786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "python -u -m amr_parsing.t5.cli.train --train \"/kaggle/working/train_v4.jsonl\"     --validation /kaggle/working/train_v4.jsonl     --report_test /kaggle/working/train_v4.jsonl     --max_source_length 512 --max_target_length 512 --batch 2 -e 40 -m vinai/bartpho-word-base     --model_type bart --output ./t5_amr/ --data_type \"amrdata\"     --task_type \"text2amr\" --val_from_epoch 2 --lr 5e-5 --do_val \"no\"\r\n"
     ]
    }
   ],
   "source": [
    "# script = \"\"\"\n",
    "# python -u -m amr_parsing.t5.cli.train --train \"./LDC2017T10/preprocessed_data/train.txt.features.nowiki\" \\\n",
    "#     --validation ./LDC2017T10/preprocessed_data/dev.txt.features.nowiki \\\n",
    "#     --report_test ./LDC2017T10/preprocessed_data/test.txt.features.nowiki \\\n",
    "#     --max_source_length 512 --max_target_length 512 --batch 8 -e 30 -m vinai/bartpho-word-base \\\n",
    "#     --model_type bart --output ./t5_amr/ --data_type \"amrdata\" \\\n",
    "#     --task_type \"text2amr\" --val_from_epoch 2\n",
    "# \"\"\"\n",
    "\n",
    "# script = \"\"\"\n",
    "# python -u -m amr_parsing.t5.cli.train --train \"./LDC2017T10/preprocessed_data/train.txt.features\" \\\n",
    "#     --validation ./LDC2017T10/preprocessed_data/dev.txt.features \\\n",
    "#     --report_test ./LDC2017T10/preprocessed_data/test.txt.features \\\n",
    "#     --max_source_length 512 --max_target_length 512 --batch 8 -e 40 -m vinai/bartpho-word-base \\\n",
    "#     --model_type bart --output ./t5_amr/ --data_type \"amrdata\" \\\n",
    "#     --task_type \"text2amr\" --val_from_epoch 20 --lr 5e-5\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "script = \"\"\"\n",
    "python -u -m amr_parsing.t5.cli.train --train \"/kaggle/working/train_v4.jsonl\" \\\n",
    "    --validation /kaggle/working/train_v4.jsonl \\\n",
    "    --report_test /kaggle/working/train_v4.jsonl \\\n",
    "    --max_source_length 512 --max_target_length 512 --batch 2 -e 40 -m vinai/bartpho-word-base \\\n",
    "    --model_type bart --output ./t5_amr/ --data_type \"amrdata\" \\\n",
    "    --task_type \"text2amr\" --val_from_epoch 2 --lr 5e-5 --do_val \"no\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"a.bash\" , \"w\") as f:\n",
    "  f.write(script)\n",
    "  f.close()\n",
    "\n",
    "!cat a.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ddb0bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:17:37.666526Z",
     "iopub.status.busy": "2024-08-28T06:17:37.665743Z",
     "iopub.status.idle": "2024-08-28T06:20:41.286200Z",
     "shell.execute_reply": "2024-08-28T06:20:41.285061Z"
    },
    "id": "OTDbHwPXXMs2",
    "outputId": "64beff34-a1d3-4f52-8b9d-f482c984f38f",
    "papermill": {
     "duration": 183.640729,
     "end_time": "2024-08-28T06:20:41.288894",
     "exception": false,
     "start_time": "2024-08-28T06:17:37.648165",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json: 100%|█████████████████████████████| 898/898 [00:00<00:00, 4.96MB/s]\r\n",
      "vocab.txt: 100%|█████████████████████████████| 895k/895k [00:00<00:00, 12.7MB/s]\r\n",
      "bpe.codes: 100%|███████████████████████████| 1.14M/1.14M [00:00<00:00, 13.5MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 3.13M/3.13M [00:00<00:00, 26.8MB/s]\r\n",
      "You are using a model of type mbart to instantiate a model of type bart. This is not supported for all configurations of models and can yield errors.\r\n",
      "pytorch_model.bin: 100%|█████████████████████| 600M/600M [00:15<00:00, 39.6MB/s]\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 1; Loss: 2.43172; Moving Loss: 4.05904;: 100%|█| 51/51 [00:04<00:00, 11.0\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 2; Loss: 2.16128; Moving Loss: 2.11990;: 100%|█| 51/51 [00:03<00:00, 13.8\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 3; Loss: 1.31385; Moving Loss: 1.72558;: 100%|█| 51/51 [00:03<00:00, 13.7\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 4; Loss: 1.11846; Moving Loss: 1.51025;: 100%|█| 51/51 [00:03<00:00, 13.9\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 5; Loss: 1.47749; Moving Loss: 1.35255;: 100%|█| 51/51 [00:03<00:00, 13.9\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 6; Loss: 1.30282; Moving Loss: 1.18766;: 100%|█| 51/51 [00:03<00:00, 13.7\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 7; Loss: 0.95696; Moving Loss: 1.08643;: 100%|█| 51/51 [00:03<00:00, 13.8\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 8; Loss: 0.78362; Moving Loss: 0.97748;: 100%|█| 51/51 [00:03<00:00, 13.4\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 9; Loss: 0.81331; Moving Loss: 0.86898;: 100%|█| 51/51 [00:03<00:00, 14.1\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 10; Loss: 0.52181; Moving Loss: 0.78120;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 11; Loss: 0.71205; Moving Loss: 0.70516;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 12; Loss: 0.80850; Moving Loss: 0.62813;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 13; Loss: 0.53031; Moving Loss: 0.56381;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 14; Loss: 0.47631; Moving Loss: 0.50582;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 15; Loss: 0.41988; Moving Loss: 0.47545;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 16; Loss: 0.46659; Moving Loss: 0.39879;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 17; Loss: 0.44856; Moving Loss: 0.37687;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 18; Loss: 0.28908; Moving Loss: 0.31705;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 19; Loss: 0.44792; Moving Loss: 0.31033;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 20; Loss: 0.24882; Moving Loss: 0.27579;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 21; Loss: 0.27236; Moving Loss: 0.26414;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 22; Loss: 0.24494; Moving Loss: 0.23884;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 23; Loss: 0.21011; Moving Loss: 0.22607;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 24; Loss: 0.18346; Moving Loss: 0.19091;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 25; Loss: 0.19717; Moving Loss: 0.17215;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 26; Loss: 0.22389; Moving Loss: 0.16689;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 27; Loss: 0.15806; Moving Loss: 0.15291;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 28; Loss: 0.16625; Moving Loss: 0.17304;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 29; Loss: 0.17846; Moving Loss: 0.14266;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 30; Loss: 0.18963; Moving Loss: 0.13589;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 31; Loss: 0.22056; Moving Loss: 0.15176;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 32; Loss: 0.15527; Moving Loss: 0.13282;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 33; Loss: 0.18238; Moving Loss: 0.11045;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 34; Loss: 0.12824; Moving Loss: 0.11727;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 35; Loss: 0.15013; Moving Loss: 0.10415;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 36; Loss: 0.13263; Moving Loss: 0.09597;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 37; Loss: 0.08668; Moving Loss: 0.08503;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 38; Loss: 0.08260; Moving Loss: 0.08405;: 100%|█| 51/51 [00:03<00:00, 14.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 39; Loss: 0.11290; Moving Loss: 0.07511;: 100%|█| 51/51 [00:03<00:00, 13.\r\n",
      "/kaggle/working/train_v4.jsonl\r\n",
      "Number of file: 1\r\n",
      "Data size: 101\r\n",
      "Combine datasets size: 101\r\n",
      "Epoch: 40; Loss: 0.05450; Moving Loss: 0.07995;: 100%|█| 51/51 [00:03<00:00, 14.\r\n"
     ]
    }
   ],
   "source": [
    "!bash a.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e18d79ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:41.710736Z",
     "iopub.status.busy": "2024-08-28T06:20:41.710266Z",
     "iopub.status.idle": "2024-08-28T06:20:44.743968Z",
     "shell.execute_reply": "2024-08-28T06:20:44.742966Z"
    },
    "papermill": {
     "duration": 3.243285,
     "end_time": "2024-08-28T06:20:44.746659",
     "exception": false,
     "start_time": "2024-08-28T06:20:41.503374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import penman\n",
    "import os\n",
    "from amr_parsing.t5.models.lg import LG\n",
    "from amr_parsing.t5.cli.inference import Inference\n",
    "from amr_utils.datasets.dataset import AMRPenman, AMRPenmanMultiTasks, add_prefix, CombineData, AMR_GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a457b9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:45.188236Z",
     "iopub.status.busy": "2024-08-28T06:20:45.187666Z",
     "iopub.status.idle": "2024-08-28T06:20:45.195963Z",
     "shell.execute_reply": "2024-08-28T06:20:45.194870Z"
    },
    "papermill": {
     "duration": 0.230735,
     "end_time": "2024-08-28T06:20:45.198394",
     "exception": false,
     "start_time": "2024-08-28T06:20:44.967659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_amr(amr_string):\n",
    "    indent = 6\n",
    "    result = ''\n",
    "    level = 0\n",
    "\n",
    "    i = 0\n",
    "    while i < len(amr_string):\n",
    "        char = amr_string[i]\n",
    "        if char == '(':\n",
    "            if i > 0 and amr_string[i - 1] != ' ':\n",
    "                result += ' '\n",
    "            result += char\n",
    "            level += 1\n",
    "            # result += '\\n' + ' ' * (level * indent)\n",
    "        elif char == ')':\n",
    "            # if amr_string[i-1] == \")\":\n",
    "            #     result += char\n",
    "            # else:\n",
    "              level -= 1\n",
    "              result +=  char\n",
    "        elif char == ':':\n",
    "            result += '\\n' + ' ' * (level * indent) + char\n",
    "        else:\n",
    "            result += char\n",
    "        i += 1\n",
    "\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4a96586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:45.636333Z",
     "iopub.status.busy": "2024-08-28T06:20:45.635923Z",
     "iopub.status.idle": "2024-08-28T06:20:45.641974Z",
     "shell.execute_reply": "2024-08-28T06:20:45.640987Z"
    },
    "papermill": {
     "duration": 0.227173,
     "end_time": "2024-08-28T06:20:45.644606",
     "exception": false,
     "start_time": "2024-08-28T06:20:45.417433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7b1f295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:46.081701Z",
     "iopub.status.busy": "2024-08-28T06:20:46.081291Z",
     "iopub.status.idle": "2024-08-28T06:20:47.497798Z",
     "shell.execute_reply": "2024-08-28T06:20:47.496796Z"
    },
    "papermill": {
     "duration": 1.640104,
     "end_time": "2024-08-28T06:20:47.500504",
     "exception": false,
     "start_time": "2024-08-28T06:20:45.860400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mbart to instantiate a model of type bart. This is not supported for all configurations of models and can yield errors.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "net = LG(\"vinai/bartpho-word-base\",\n",
    "             max_source_length=512,\n",
    "             max_target_length=512,\n",
    "             model_type=\"bart\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55bc7837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:47.936150Z",
     "iopub.status.busy": "2024-08-28T06:20:47.935708Z",
     "iopub.status.idle": "2024-08-28T06:20:49.483380Z",
     "shell.execute_reply": "2024-08-28T06:20:49.482275Z"
    },
    "papermill": {
     "duration": 1.76977,
     "end_time": "2024-08-28T06:20:49.485897",
     "exception": false,
     "start_time": "2024-08-28T06:20:47.716127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LG(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(64001, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(64001, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): BartScaledWordEmbedding(64001, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=64001, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.model.load_state_dict(torch.load(\"./t5_amr/multitask.model\", map_location=torch.device(device))[\"model_state_dict\"])\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b79a9c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:49.940364Z",
     "iopub.status.busy": "2024-08-28T06:20:49.939967Z",
     "iopub.status.idle": "2024-08-28T06:20:49.946070Z",
     "shell.execute_reply": "2024-08-28T06:20:49.944801Z"
    },
    "papermill": {
     "duration": 0.238673,
     "end_time": "2024-08-28T06:20:49.948597",
     "exception": false,
     "start_time": "2024-08-28T06:20:49.709924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c15aa5e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:50.446196Z",
     "iopub.status.busy": "2024-08-28T06:20:50.445259Z",
     "iopub.status.idle": "2024-08-28T06:20:50.451333Z",
     "shell.execute_reply": "2024-08-28T06:20:50.450108Z"
    },
    "papermill": {
     "duration": 0.255629,
     "end_time": "2024-08-28T06:20:50.453658",
     "exception": false,
     "start_time": "2024-08-28T06:20:50.198029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_path = \"./test_v4.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baa58b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:51.016712Z",
     "iopub.status.busy": "2024-08-28T06:20:51.016299Z",
     "iopub.status.idle": "2024-08-28T06:20:51.023436Z",
     "shell.execute_reply": "2024-08-28T06:20:51.022466Z"
    },
    "papermill": {
     "duration": 0.325552,
     "end_time": "2024-08-28T06:20:51.025975",
     "exception": false,
     "start_time": "2024-08-28T06:20:50.700423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- đúng thế , cáo nói .', '- nhưng cậu sẽ khóc ! ông hoàng nhỏ nói .', '- thế thì cậu chẳng được gì cả !', '- được chứ , cáo nói , là do cái màu vàng của lúa mì ấy .', 'rồi nó nói thêm :', '- hãy trở lại thăm những đoá hồng đi .', 'cậu sẽ thấy đoá hoa của cậu là duy nhất trên đời .', 'rồi cậu hãy lại đây từ biệt tớ , tớ sẽ làm quà cho cậu một điều bí mật .', 'ông hoàng nhỏ đi thăm lại những đoá hoa hồng .', '- các cô chẳng giống chút nào với đoá hồng của tôi , các cô chưa là gì cả , em bảo các bông hồng .', 'chưa ai cảm hoá các cô , các cô cũng chưa cảm hoá ai .', 'các cô giống như con cáo của tôi trước kia .', 'nó chỉ là một con cáo giống như trăm nghìn con cáo .', 'và các bông hồng hết sức lúng túng .', '- các cô đẹp , nhưng các cô trống rỗng , em nói với họ .', 'người ta không thể chết vì các cô được .', 'phải , đoá hồng của tôi , một người qua đường tầm thường tưởng là nàng giống các cô .', 'nhưng đối với tôi thì nàng quan trọng hơn tất cả các cô , bởi vì chính là nàng mà tay tôi đã tưới .', 'bởi vì chính là nàng mà tôi đã đặt chính dưới bầu kính .', 'bởi vì chính là nàng mà tôi đã che bằng tấm bình phong .', 'bởi vì là nàng mà tôi đã bắt những con sâu ( trừ hai ba con dành để thành bướm ) .', 'bởi vì chính là nàng mà tôi đã ngồi nghe than thở , hay tán hươu tán vượn , hay đôi khi cả lặng im nữa .', 'bởi vì đó là đoá hồng của tôi .', 'rồi em trở lại chỗ con cáo :', '- từ biệt , em nói .', '- từ biệt , cáo nói .', 'đây là cái bí mật của tớ .', 'nó đơn giản thôi : người ta chỉ nhìn thấy thật rõ ràng bằng trái tim .', 'cái cốt yếu thì con mắt không nhìn thấy .', '- cái cốt yếu thì con mắt không nhìn thấy , ông hoàng nhỏ lặp lại , để mà ghi nhớ .', '- chính thời giờ cậu đã mất cho đoá hồng của cậu làm cho đoá hồng của cậu trở nên quan trọng đến thế .', '- chính thời giờ tôi đã mất cho bông hồng của tôi ...', 'ông hoàng nhỏ nói , để mà ghi nhớ .', '- loài người đã quên mất chân lí này , cáo nói .', 'nhưng cậu không được quên .', 'cậu trở nên mãi mãi có trách nhiệm về những gì cậu đã cảm hoá .', 'cậu có trách nhiệm đối với hoa hồng của cậu .', '- tôi có trách nhiệm với hoa hồng của tôi ...', 'ông hoàng nhỏ lặp lại để mà ghi nhớ .', '- xin chào , ông hoàng nhỏ nói .', '- xin chào , người bẻ ghi nói .', '- ông làm gì ở đây ? ông hoàng nhỏ hỏi .', '- ta xếp hành khách , thành từng gói nghìn người một , người bẻ ghi nói .', 'ta hướng dẫn những chuyến tàu chở chúng đi , lúc về bên phải , lúc về bên trái .', 'rồi một con tàu nhanh sáng choang , gầm như sấm , làm căn phòng gác ghi rung lên .', '- họ vội quá , ông hoàng nhỏ nói .', 'họ tìm cái gì vậy ?', '- chính người lái tàu cũng không biết , người gác ghi nói .', 'và lại gầm lên , từ hướng ngược lại , một chuyến tàu nhanh thứ hai sáng choang .', '- họ đã trở về rồi ư ? ông hoàng nhỏ hỏi ...', '- không phải những người khi nãy .', 'họ đổi chỗ đấy .', '- họ không bằng lòng chỗ của họ ?', '- người ta không bao giờ bằng lòng chỗ của mình cả !', 'và lại tiếng ầm ầm của chiếc tàu nhanh thứ ba sáng choang .', '- họ đuổi theo những hành khách lúc nãy phải không ? ông hoàng nhỏ hỏi .', '- họ chẳng đuổi theo cái gì hết , người bẻ ghi nói .', 'họ ngủ gật hoặc ngồi ngáp vặt trong đó .', 'chỉ có những đứa trẻ là dán mũi vào cửa kính thôi .', '- chỉ có những đứa trẻ là biết mình tìm cái gì , ông hoàng nhỏ nói , chúng mất thì giờ vì một con búp bê bằng giẻ rách , và con búp bê ấy trở nên quan trọng lắm , ai lấy đi của chúng , chúng sẽ khóc ...', '- chúng thật may mắn , người bẻ ghi nói .', '- xin chào , người lái buôn nói .', 'ấy là một người bán loại thuốc có thể làm cho đỡ khát .', 'mỗi lần uống một viên , và người ta sẽ thấy không cần phải uống nước nữa .', '- tại sao ông bán thứ đó ? ông hoàng nhỏ hỏi .', '- đây là một sự tiết kiệm lớn về thời giờ , người lái buôn nói .', 'các nhà chuyên môn đã có tính toán .', 'mỗi tuần lễ ta sẽ tiết kiệm được năm mươi ba phút .', '- thế người ta dùng năm mươi ba phút ấy để làm gì ?', '- muốn làm gì thì làm ...', '\" Ta , ông hoàng nhỏ nghĩ thầm , nếu ta có năm mươi ba phút để làm gì thì làm , ta sẽ bước thật nhẹ nhàng đến một cái nguồn nước ... \"', 'lúc đó là ngày thứ tám từ khi tôi bị hỏng máy trong sa mạc , và tôi vừa nghe câu chuyện người lái buôn vừa uống giọt nước cuối cùng trong số nước dự trữ .', 'tôi nói với ông hoàng nhỏ , các kỷ niệm của em thật là đẹp , nhưng anh vẫn chưa chữa được máy bay của anh , anh chẳng còn gì để uống nữa , và anh cũng sẽ hạnh phúc lắm đấy , nếu bây giờ anh cũng có thể bước thật nhẹ nhàng đến một cái nguồn nước !', '- bạn cáo của tôi ơi , em nói ...', '- chú em bé bỏng ơi , bây giờ còn chồn cáo gì nữa !', '- sao thế ?', '- bởi vì ta sắp chết khát ...', 'em không hiểu lý lẽ của tôi , em trả lời tôi :', '- có được một người bạn là tuyệt chứ , dù cho ta sắp chết đi nữa .', 'tôi , tôi rất hài lòng được có bạn cáo của tôi ...', 'em không lường được hiểm nguy , tôi nghĩ thầm .', 'em không hề đói hay khát .', 'một chút mặt trời cũng đủ cho em rồi .', 'nhưng em nhìn tôi và trả lời điều tôi vừa nghĩ :', '- tôi cũng đang khát ...', 'ta đi tìm một cái giếng đi ...', 'tôi phác một cử chỉ mệt mỏi : thật là phi lý khi đi tìm một cái giếng , một cách hú hoạ , ở trong sa mạc mênh mông .', 'tuy thế , chúng tôi vẫn bước đi .', 'khi chúng tôi bước đi , im lặng , giờ này qua giờ khác , màn đêm buông xuống và các vì sao bắt đầu toả sáng .', 'tôi nhìn mọi vật như trong cơn mê , đầu nóng bừng vì khát .', 'những lời ông hoàng bé nhỏ nói nhảy múa trong trí nhớ tôi .', '- em , em cũng khát hay sao ?', 'nhưng em không trả lời câu tôi hỏi .', 'em chỉ nói giản dị :', '- nước cũng có thể tốt lành cho trái tim ...', 'tôi không hiểu câu em đáp nhưng tôi im lặng ...', 'tôi biết là không nên hỏi em .', 'em thấy mệt .', 'em ngồi xuống .', 'tôi ngồi xuống cạnh em .', 'và sau một lát im lặng , em lại nói :', '- các ngôi sao đẹp , là do ở đó có một bông hoa mà người ta không nhìn thấy ...', 'tôi đáp lại \" đúng thế \" và tôi nhìn , không nói , những gợn sóng của cát dưới ánh trăng .', '- sa mạc thật đẹp , em nói thêm ...', 'điều này đúng .', 'bao giờ tôi cũng yêu sa mạc .', 'ta ngồi trên một đụn cát .', 'ta không trông thấy gì hết .', 'ta không nghe thấy gì hết .', 'nhưng có một cái gì đó toả sáng trong lặng lẽ ...', '- cái đã tô điểm cho sa mạc , ông hoàng nhỏ nói , là nó ẩn giấu một cái giếng ở nơi nào đó ...', 'tôi kinh ngạc vì bỗng nhiên hiểu ra cái ánh sáng huyền bí ấy của cát .', 'khi còn là một cậu bé tôi ở trong một ngôi nhà cổ , và có một truyền thuyết là ở ngôi nhà cổ này có chôn một kho báu .', 'tất nhiên , chưa ai tìm ra kho báu đó , có lẽ cũng chưa ai thử đi tìm .', 'nhưng nó đã làm cho ngôi nhà trở nên thành tiên .', 'cái nhà của tôi có giấu trong đáy trái tim của nó một điều bí mật .', '- phải , tôi nói với ông hoàng nhỏ .', 'dù là ngôi nhà , ngôi sao hay sa mạc , cái làm chúng ta trở nên đẹp thì không thể nhìn thấy !', '- tôi rất hài lòng , em nói , vì ông cũng đồng ý với bạn cáo của tôi .', 'khi ông hoàng nhỏ bé thiu thiu ngủ , tôi bế em lên vòng tay và lại lên đường .', 'lòng tôi xúc động .', 'tôi có cảm giác như đang giữ một kho báu mong manh .', 'tôi có cảm giác như trên Trái Đất này không có gì mong manh hơn .', 'tôi nhìn , dưới ánh sáng trăng , vầng trán xanh xao ấy , đôi mắt nhắm nghiền , những lọn tóc run rẩy trước gió , và tôi nghĩ thầm : cái mà ta thấy đây chỉ là một cái vỏ .', 'cái quan trọng nhất thì không nhìn thấy được ...', 'khi đôi môi hé mở của em thoáng một nụ cười , tôi lại tự nhủ : cái làm cho ta xúc động mạnh đến thế về ông hoàng bé nhỏ đang ngủ này , đó là lòng chung thuỷ của em đối với một đoá hoa hồng , ấy là hình ảnh một đoá hồng rực sáng nơi em như một ngọn đèn , cả trong khi em ngủ ...', 'và tôi càng thấy em còn mong manh hơn .', 'ta phải hết sức che chở cho những ngọn đèn : một ngọn gió có thể thổi tắt được ...', 'và , cứ bước đi như vậy , tôi tìm ra cái giếng vào lúc rạng đông .', '- con người , ông hoàng nhỏ nói , họ chui vào các chuyến tàu nhanh , nhưng họ chẳng biết mình tìm kiếm thứ gì .', 'thế mà họ cứ cuống quít lên và quay cuồng ...', 'và em nói thêm :', '- nào có ích gì ...', 'cái giếng mà chúng tôi đến , nó không giống các giếng ở sa mạc Sahara .', 'các giếng Sahara chỉ đơn giản là những cái lỗ đào trong cát .', 'còn cái này giống như giếng làng .', 'nhưng ở đấy chẳng có ngôi làng nào , và tôi tưởng mình đang mơ .', '- lạ thật , tôi nói với ông hoàng nhỏ , mọi cái đều có sẵn : cái ròng rọc , cái gầu và sợi dây ...', 'em cười , sờ sợi dây , lăn cái ròng rọc .', 'và cái ròng rọc liền rên một tiếng , giống như tiếng rên của cái chong chóng chỉ hướng gió sau một hồi gió ngủ yên lâu quá .', '- ông nghe thấy không , ông hoàng nhỏ nói , ta đánh thức cái giếng này và nó hát ...', 'tôi không muốn em gắng sức :', '- để anh , tôi nói , nó nặng quá đối với em .', 'chậm rãi , tôi kéo gàu lên thành giếng .', 'tôi đặt nó chắc chắn trên đó .', 'trong tai tôi vẫn còn văng vẳng tiếng hát của cái ròng rọc , và trong nước vẫn còn run rẩy , tôi thấy mặt trời run lên .', '- tôi khát thứ nước này đây , ông hoàng nhỏ nói .', 'cho tôi uống đi ...', 'và tôi hiểu ra em tìm kiếm cái gì .', 'tôi nhắc chiếc gầu lên kề môi em .', '']\n"
     ]
    }
   ],
   "source": [
    "with open(test_path, \"r\") as f:\n",
    "  test_data = f.read()\n",
    "  # pattern = r\"^#::snt.*\"\n",
    "\n",
    "  # Extract lines using regex\n",
    "  # test_data = re.findall(pattern, test_data, re.MULTILINE)\n",
    "  test_data = test_data.split(\"\\n\")\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e5f905c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:51.485757Z",
     "iopub.status.busy": "2024-08-28T06:20:51.484701Z",
     "iopub.status.idle": "2024-08-28T06:20:51.506076Z",
     "shell.execute_reply": "2024-08-28T06:20:51.504895Z"
    },
    "papermill": {
     "duration": 0.257971,
     "end_time": "2024-08-28T06:20:51.513668",
     "exception": false,
     "start_time": "2024-08-28T06:20:51.255697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e28248a314046828dcec40e32aa9704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import autonotebook\n",
    "pbar = autonotebook.tqdm(range(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eccf4848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:51.950631Z",
     "iopub.status.busy": "2024-08-28T06:20:51.950106Z",
     "iopub.status.idle": "2024-08-28T06:20:51.955052Z",
     "shell.execute_reply": "2024-08-28T06:20:51.953983Z"
    },
    "papermill": {
     "duration": 0.225965,
     "end_time": "2024-08-28T06:20:51.957148",
     "exception": false,
     "start_time": "2024-08-28T06:20:51.731183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e787bda7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:20:52.419438Z",
     "iopub.status.busy": "2024-08-28T06:20:52.418957Z",
     "iopub.status.idle": "2024-08-28T06:22:03.167578Z",
     "shell.execute_reply": "2024-08-28T06:22:03.166483Z"
    },
    "papermill": {
     "duration": 70.989908,
     "end_time": "2024-08-28T06:22:03.169659",
     "exception": false,
     "start_time": "2024-08-28T06:20:52.179751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_fname = \"result_v4_test.txt\"\n",
    "result = \"\"\n",
    "for i, text in enumerate(test_data):\n",
    "  pbar.update(1)\n",
    "  if text == \"\":\n",
    "    continue\n",
    "  # add_prefix(sentence, AMR_GENERATION)\n",
    "  predict_amrs = net.generate(add_prefix([text], AMR_GENERATION), max_decoder_batch_size=256,)\n",
    "  predict_amrs = re.sub(r\"\\( \", \"(\", predict_amrs[0])\n",
    "  predict_amrs = re.sub(r\" \\)\", \")\", predict_amrs)\n",
    "  predict_amrs = format_amr(predict_amrs)\n",
    "  # print(predict_amrs)\n",
    "  # break\n",
    "  if predict_amrs is not None:\n",
    "    result = result + f\"#::id {i}\\n#::snt {text}\\n\\n\" + predict_amrs + \"\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3556b3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T06:22:03.590476Z",
     "iopub.status.busy": "2024-08-28T06:22:03.589943Z",
     "iopub.status.idle": "2024-08-28T06:22:03.595624Z",
     "shell.execute_reply": "2024-08-28T06:22:03.594771Z"
    },
    "papermill": {
     "duration": 0.220741,
     "end_time": "2024-08-28T06:22:03.597875",
     "exception": false,
     "start_time": "2024-08-28T06:22:03.377134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(gen_fname, \"w\") as f:\n",
    "  f.write(result)\n",
    "  f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5554755,
     "sourceId": 9263523,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 339.511087,
   "end_time": "2024-08-28T06:22:04.926334",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-28T06:16:25.415247",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "080b430080a04683ad78eae70c6825d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a371cc3158a49738790443147e6e09b",
       "placeholder": "​",
       "style": "IPY_MODEL_663baf073ba44ed2bd3338fa5cb566dc",
       "value": " 151/151 [01:11&lt;00:00,  2.12it/s]"
      }
     },
     "10a9e883e52642b984ddab7f918cc202": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "381cd7938dfa44759e94899cf2720fa3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3a371cc3158a49738790443147e6e09b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e28248a314046828dcec40e32aa9704": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c982f8fafe96477cb67676d07e49af52",
        "IPY_MODEL_f09544177bce48848cd26de38417cba8",
        "IPY_MODEL_080b430080a04683ad78eae70c6825d1"
       ],
       "layout": "IPY_MODEL_10a9e883e52642b984ddab7f918cc202"
      }
     },
     "663baf073ba44ed2bd3338fa5cb566dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8e70770d86604a2c9f3bb1fe36f8f3a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5b3c10370444a5ba30c445006010711": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c2d457868e5d4cf582abce4d21507683": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c982f8fafe96477cb67676d07e49af52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c2d457868e5d4cf582abce4d21507683",
       "placeholder": "​",
       "style": "IPY_MODEL_a5b3c10370444a5ba30c445006010711",
       "value": "100%"
      }
     },
     "f09544177bce48848cd26de38417cba8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8e70770d86604a2c9f3bb1fe36f8f3a9",
       "max": 151.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_381cd7938dfa44759e94899cf2720fa3",
       "value": 151.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
